\chapter{Appendix}

\section{Computing $C_{j, i}$ for Bernoulli means}\label{section:bernoulli_c}

We assume that every arm $l$ follows a Bernoulli distribution with parameter $\theta_l$. The option space for rewards being $\{0, 1\}$, they are discretely distributed. Let us reiterate the definition of the KL divergence for discrete distributions:
\[d(p || q) = \sum_{y \in Y}p(y) \log(\frac{p(y)}{q(y)})\]
where $Y$ corresponds the option space for the outcome.
Instantiating this definition with our scenario, we observe that $Y=\{0, 1\}$, $p(y=1)=\theta_l$  as well as $p(y=0)=1-\theta_l$, for a given arm $l$.

This yields:
\begin{align}
  d(\theta_l||x) = \theta_l \log(\frac{\theta_l}{x}) + (1-\theta_l) \log(\frac{1-\theta_l}{1-x}) \label{eq:bernoulli_kl}
\end{align}
Recall that for computing $C_{j, i}$, we seek to minimize the expression from \eqref{eq:C} $x \in \mathbb{R}$. Hence we are interested in the derivative of \eqref{eq:bernoulli_kl} with respect to $x$.
\begin{align}
  \diff{(d(\theta_l||x)}{x} &= - \frac{\theta_l}{x} + \frac{(1-\theta_l)}{1-x}
\end{align}
Drawing from the minimization problem of $C_{j,i}$ for given $j$, $i$, $\psi_j$ and $\psi_i$, we define $f(x) = \psi_j d(\theta^*_{j} || x) + \psi_i d(\theta_{i}^* ||x)$. We proceed by deriving $f$ with respect to $x$ and setting it to 0.
\begin{align}
  \diff{f(x)}{x} &= \psi_j(\frac{\theta_j}{x} + \frac{(1-\theta_j)}{1-x}) + \psi_i( \frac{\theta_i}{x} + \frac{(1-\theta_i)}{1-x}) \\
  &= -\frac{1}{x}(\psi_j\theta_j + \psi_i\theta_i) + \frac{1}{1-x}(\psi_j(1 - \theta_j) + \psi_i(1 - \theta_i)) \\
  \diff{f(x)}{x} = 0 &\Rightarrow (1-x_0)(\psi_j\theta_j + \psi_i\theta_i) = x_0(\psi_j(1 - \theta_j) + \psi_i(1 - \theta_i))\\
  &\Rightarrow x_0((\psi_j\theta_j + \psi_i\theta_i) + (\psi_j(1 - \theta_j) + \psi_i(1 - \theta_i))) = (\psi_j\theta_j + \psi_i\theta_i)\\
  &\Rightarrow x_0 = \frac{\psi_j\theta_j + \psi_i\theta_i}{(\psi_j\theta_j + \psi_i\theta_i) + (\psi_j(1 - \theta_j) + \psi_i(1 - \theta_i))} \\
  &\Rightarrow x_0 = \frac{\psi_j\theta_j + \psi_i\theta_i}{\psi_j + \psi_i}
\end{align}
Hence we have a very intuitive analytical solution for $x$: it is an average of the means of $j$ and $j$, weighted by their respective measurement allocation.

\section{Facts about the one-dimensional exponential family}
We reiterate some of the facts mentioned by Russo and add some general properties.
\begin{itemize}
  \item They have a scalar parameter $\theta$.
  \item They come with the definition of function $T(x)$, $\nu(\theta)$, $h(x)$ and $A(\theta)$. $T$ corresponds to the sufficient statistic and $A$ to the log-partition function.
  \item The probability density function is defined by:
    \[f_X(x|\theta) = h(x) \exp(\nu(\theta) T(x) - A(\theta))\]
  \item Their log-partition-function $A(\theta)$ is strictly convex and differentiable.
  \item They have conjugate priors.
  \item Their mean equals $A'(\theta) = \int T(y)p(y|\theta)d\nu(y)$. In the typical case of $T(y)=y$, this yields that $A'(\theta) = \mu(\theta)$.
  \item The KL divergence equals:
    \begin{align}
        d(\theta||\theta') = (\theta - \theta')A'(\theta) - A(\theta) + A(\theta')\label{eq:exponential_kl}
    \end{align}
  \item The KL divergence satisfies:
    \begin{align}
      &\theta'' > \theta' \geq \theta \Rightarrow d(\theta||\theta'') > d(\theta||\theta') \label{eq:kl_monotonicity}\\
      &\theta'' < \theta' \leq \theta \Rightarrow d(\theta||\theta'') < d(\theta||\theta')
    \end{align}
\end{itemize}


\section{Useful technical statements}\label{section:technical_proofs}
  \begin{lemma}\label{lemma_lim_sup}
  \[ \limsup_{n \rightarrow \infty} f(n) \leq \Gamma \Rightarrow \exists \epsilon_n > 0, s.t. \epsilon_n \rightarrow 0, \forall n\ f(n) \leq \Gamma + \epsilon_n\]
  \end{lemma}
  \begin{proof}
    Given the assumption, we can do a case distinction:
    \begin{itemize}
    \item $g(n) = \Gamma$ and $\sup f(n)$ intersect in $n_0$ \\
    For $n < n_0$, we have $\sup f(n) > \Gamma$. As $\sup f(n)$ is a decreasing function, there are positive, decreasing $\epsilon_n$ such that $\sup f(n) \leq \Gamma + \epsilon_n$. \\
    For $n \geq n_0$, we have $\sup f(n) \leq \Gamma$. Hence for such n, $\epsilon_n = 0$ would already satisfies the constraint. Hence any $\epsilon_n \rightarrow 0$, fulfills the inequality.

    Consequently, there is a positive $\epsilon_n \rightarrow 0$, s.t. $\forall n: f(n) \leq \sup f(n) \leq \Gamma + \epsilon_n$.

    \item $g(n) = \Gamma$ and $\sup f(n)$ do not intersect \\
    By our assumption we have $\forall n\ \sup f(n) \leq \Gamma$.
    Hence, by setting $\epsilon_n$ to equal 0 for all $n$, we have a positive $\epsilon_n \rightarrow 0$, such that $f(n) \leq \sup f(n) \leq \Gamma + \epsilon_n$.
    \end{itemize}
  \end{proof}
