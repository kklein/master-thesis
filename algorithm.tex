\chapter{Top-2$m$ XOR Thompson sampling}\label{chapter:algorithm}

In this chapter we will both present and analyze our suggested generalization of
Russo's Top-Two Thompson sampling: Top-2$m$ XOR Thompson sampling. The
underlying idea still revolves around repeatedly applying Thompson sampling
until two different candidates are at hand.

\Cref{section:algorithm} introduces the algorithm and provides some explanations
on how the algorithm generalizes Russo's TTTS for best arm identification. It
also introduces the constraint $\psi_{S^*} = \frac{1}{2}$. Subsequently,
\Cref{section:analysis} analyzes properties of the algorithm. In particular it
presents bounds on the measurement plan, states general consequences of finite
measurement and discusses implications of underallocation and overallocation. We
conjecture that all of those are very useful to show that this algorithm's
measurement plan converges to the optimal constrained measurement plan
$\psi^{\frac{1}{2}*}$. We emphasize that this is an \emph{adaptive} algorithm,
in stark contrast to the fixed, optimal allocation $\psi^{\frac{1}{2}*}$. In
other words, instead of only $\psi$ influencing $\Pi_n$, they now both influence
each other. Proofs for those statements are provided in
\Cref{section:txts_proofs}. Additionally, we provide some empirical results in
\Cref{section:empirical_behavior}.

\section{Algorithm}\label{section:algorithm}
As a generalization of Russo's TTTS, the main difference lies in the fact that
candidates are sets. Hence Thompson sampling is repeated until set inequality is
reached. However, we can only ever sample individual arms, i.e. not sets. Hence
in this set scenario, we still need a mechanism to select a single arm from a
set. Therefore, when generalizing Russo's approach, the central and unavoidable
question arises: 'How to select a single arm from two unequal set candidates?'

According to a suggestion Russo gives in his outlook, we decided to tackle this
question by splitting it up into two steps.

First, given candidates $S_1$ and $S_2$ with $S_1 \neq S_2$, we compute the set
of elements which are contained in exactly one of both sets, i.e. the XOR of
both sets. Naturally, this restricted set is of cardinality at least 2.
Intuitively it points us towards arms which are 'uncertain'. Observe that arms
which are clearly suboptimal are very unlikely to appear in either $S_1$ or
$S_2$ through Thompson sampling. At the same time, arms that are clearly optimal
are very likely to appear in both $S_1$ and $S_2$ through Thompson sampling.
Hence arms that appear in only either of them are neither clearly optimal nor
clearly suboptimal.

As mentioned before, the XOR of $S_1$ and $S_2$ will always contain at least two
elements. Hence we need to define an approach on how to select from the XOR.
According to both Russo's suggestion as well as Occam's razor, we opted for
uniform selection.

We believe that the applying a XOR to both candidates is essential for the
correctness of the algorithm, whereas the uniform sampling from the XOR of both
candidates could possibly be substituted by other distributions. We would expect
such a change to preserve correctness, as long as every arm of the XOR is
sampled with strictly positive probability. Yet, it would likely alter the
hyperparamter $\beta = \psi_{S^*}$.

We present the approach to sample an arm in step $n$ in \Cref{alg:TXTS}.
This approach can be repeated either for a fixed amount of samples or until a
specific confidence level is reached. Note that the confidence level can be
approximated in every step. The confidence level is equal to $\Pi_n(\Theta_S)$
where $S = \argmax_{S'} \Pi_n(\Theta_{S'})$. Concretely, this quantity can be
estimated by drawing samples from $\Pi_n$ and identifying for which set $S$ the
fraction $\frac{\text{\# samples in which $S$ is optimal}}{\#samples}$ is
largest. The fraction of this $S$ approximates the confidence level.
\begin{algorithm}[H]
  \caption{Given a posterior $\Pi_{n-1}$ in step $n$}
  \label{alg:TXTS}
  \begin{algorithmic}
    \State $\hat{\theta} \sim \Pi_{n-1}$
    \State $S_1 =$ top-$m(\hat{\theta})$
    \Repeat
      \State $\hat{\theta} \sim \Pi_{n-1}$
      \State $S_2 = $ top-$m(\hat{\theta})$
    \Until{$S_1 \neq S_2$}
    \State $I_n \sim \mathcal{U}(S_1 \oplus S_2)$
    \State Play $I_n$, observe reward and update posterior
  \end{algorithmic}
\end{algorithm}
We expect this algorithm to induce a measurement plan $\psi$ such that
$\psi_{S^*} = \frac{1}{2}$. This expectation gives rise to the constrained
optimization from the previous chapter. Note that this constraint is a
consequence of the design decision of which operation to apply to candidates
$S_1$ and $S_2$.

\section{Analysis}\label{section:analysis}
Starting off by repeating a general proposition from Russo, we continue by a statement about the implications of finite measurement. Later establish bounds and equalities on the measurement plan $\psi$ of the
algorithm.

\begin{proposition}[Russo: Proposition 4]\label{proposition:russo_4}
  Assuming $\theta \in [\underline{\theta}, \bar{\theta}]^k$, for any $l \in
      [k]$ if $\psi_{n, l} \rightarrow \infty$ then for all $\epsilon > 0$:
  \begin{align}
    \Pi_n(\{\theta \in \Theta | \theta_l \notin (\theta_l^* - \epsilon,
        \theta_l^* + \epsilon)\}) \rightarrow 0
  \end{align}
  with probability 1. If $\mathcal{I} = \{l: \sum_{n=1}^\infty \psi_{n, l} <
  \infty\} \neq \emptyset$ then
  \begin{align}
    \inf_{n \in \mathbb{N}} \Pi_n(\{\theta \in \Theta | \theta_i \in
        (\theta_l', \theta_l'') \forall l \in \mathcal{I}\}) > 0
  \end{align}
  for any collections of open intervals $(\theta_l', \theta_l'') \subset
  (\underline{\theta}, \bar{\theta})$ ranging over $l \in \mathcal{I}$.
\end{proposition}
Directly using \Cref{proposition:russo_4}, the following lemma formalizes three
seemingly intuitive statements. First, it shows that for each arm that is
sampled infinitely many times, the estimated mean will converge to its true
mean. Second, it demonstrates that if every arm is sampled infinitely often, the
posterior will put all of its mass on parameters with $S^*$ as top-$m$ arms.
Third, it argues that as long as some arms have only been granted finite
measurement, they can't be ruled out from being optimal.
\begin{lemma}\label{lemma:finite_measurement}
  For $\mathcal{I} = \{l: \sum_{n=1}^\infty \psi_{n, l} < \infty\}$ it holds
      that:
  \begin{itemize}
    \item $\forall l \notin \mathcal{I}, \forall \epsilon: \Pi_n(\{\theta:
        \theta_l \in (\theta^*_l - \epsilon, \theta^*_l + \epsilon)\})
        \rightarrow 1$
    \item $\mathcal{I} = \emptyset \Rightarrow
    \alpha_{n, S} \rightarrow \begin{cases}
      1 & \text{if } S = S^*\\
      0 & \text{if } S \neq S^*
    \end{cases}$
    \item $\forall S \subset \mathcal{I}: \liminf_{n \rightarrow \infty}
        \alpha_{n, S} > 0$
  \end{itemize}
\end{lemma}
Going on to expressing the measurement plan, we observe that a closed-form
equality is not easy to obtain. In comparison to Russo's case, it is much harder
to construct a closed-form probability of arm $l$ being sampled in a given step
as there are many more different scenarios. Vaguely speaking, the XORs can be of
very different kinds.

In light of this, we offer both lower and upper bounds on the measurement plan of
a single arm. The overall idea is to case-distinguish if $l$ lies set $S_1$ or
$S_2$. Facing the probability of $l$ being selected given its belonging to the
XOR, we leverage the knowledge of the uniform draw. Hence we have $\Pr[I_n = l|l
\in S_1 \oplus S_2] = \frac{1}{|S_1 \oplus S_2|}$. The cardinality of the XOR
can be trivially bounded from below by 2 or from above by 2$m$. Note that these
inequalities are tight with respect to $n$ as $m$ can be seen as a constant.
\begin{proposition}\label{proposition:measurement_plan_arm}
  \begin{align}
    \psi_{n, l} &\leq \frac{1}{2}((1 - \alpha_{n, l}) \sum_{S: l \in S}
        \frac{\alpha_{n, S}}{1 - \alpha_{n, S}} + \alpha_{n, l} \sum_{S': l
        \notin S'} \frac{\alpha_{n, S'}}{1 - \alpha_{n, S'}}) \\
    \psi_{n, l} &\geq \frac{1}{m}((1 - \alpha_{n, l}) \sum_{S: l \in S}
        \frac{\alpha_{n, S}}{1 - \alpha_{n, S}} + \alpha_{n, l} \sum_{S': l
        \notin S'} \frac{\alpha_{n, S'}}{1 - \alpha_{n, S'}}) \\
  \end{align}
\end{proposition}
Having established bounds on the measurement plan for individual arms, we
proceed to establish two equalities on the measurement plan for sets. In this
case we neither propose a closed-form solution nor a bound. Rather, we veil some
of the uncertainty with a probability term, which turns out to be sufficiently
expressive for some of the later applications.
\begin{proposition}\label{proposition:measurement_pan_set}
  \begin{align}
    \psi_{n, S} &= \frac{1}{2} \alpha_{n, S} +  \Pr[I_n \in S | S_1 \neq S] (1 - \alpha_{n, S}) \\
    \psi_{n, S} &= \frac{\alpha_{n, S}}{2} +  \frac{\alpha_{n, S}}{2} \sum_{S'\neq S} \frac{\alpha_{n, S'}}{1 - \alpha_{n, S'}} + \Pr[S_1, S_2 \neq S \wedge I_n \in S]
  \end{align}
\end{proposition}
This form of $\psi_{n, S}$ allows us to deduct that the algorithm collects infinite measurement on every arm given infinite samples.
\begin{lemma}\label{lemma:infinite_measurement}
  \begin{align}
    \sum_{n \in \mathbb{N}} \psi_{n, S} \rightarrow \infty
  \end{align}
\end{lemma}
Recalling \Cref{lemma:finite_measurement}, this is particularly interesting as it signals that for $n \rightarrow \infty$ we have that $\mathcal{I} = \emptyset$. Therefore $\alpha_{n, S^*} \rightarrow 1$ holds true, too.
Having established $\psi_{n, S}$ as function of $\alpha_{n, S}$, we continue by analyzing the effect of $\alpha_{n, S} \rightarrow 1$ on $\psi_{n, S}$, for general $S$, followed by the application on $S^*$.
\begin{lemma}\label{lemma:psi_convergence}
  \begin{align}
    \alpha_{n, S} \rightarrow 1 \Rightarrow \psi_{n, S} \rightarrow \frac{1}{2}
  \end{align}
\end{lemma}
This result confirms our hunch about the constraint induced of the algorithm:
once it becomes 'sure' of the optimality of a set $S$, it will sample arms from
it with probability $\frac{1}{2}$. As we have previously argued, we have that
$\alpha_{n, S^*} \rightarrow 1$ and therefore $\psi_{n, S^*} \rightarrow
\frac{1}{2}$.

Moreover, once we know that $\alpha_{n, S^*} \rightarrow 1$, we can simplify the
bound from \Cref{proposition:measurement_plan_arm}.
\begin{lemma}\label{lemma:measurement_plan_bound_max}
  If $\alpha_{n, S^*} \rightarrow 1$, then for all $i \notin S^*$ and for all
  $j \in S^*$:
  \begin{align}
    \psi_{n, i} &\leq \frac{\alpha_{n, i}}{\max_{S' \neq S^*} \alpha_{n, S'}} \\
    \psi_{n, j} &\leq \frac{1 - \alpha_{n, j}}{\max_{S' \neq S^*} \alpha_{n,
        S'}}
  \end{align}
\end{lemma}

\begin{lemma}\label{lemma:fraction_bound}
  If $\bar{\psi}_{n, S^*} \rightarrow 1/2$, then there exists a sequence
  $\epsilon_n \rightarrow 0$ such that $\forall i \notin S^*, \forall n \in
  \mathbb{N}$:
  \begin{align}
    \psi_{n, i} \leq \exp\{-n( \min_{j \in S^*} C_{j, i}(\bar{\psi}_j,
        \bar{\psi}_i) - \Gamma^*_{\frac{1}{2}} - \epsilon_n) \}
  \end{align}
\end{lemma}

\section{Empirical behavior}\label{section:empirical_behavior}
%What true distributions are assumed?
%What prior and posterior distributions are assumed?
%How is C computed?
%How is alpha computed, as it is defined via a huge integral?
%How is psi computed, as there is no closed form?

TODO: Talk about updates of priors.

For the following empirical results we assumed the rewards of arms to follow
Bernoulli distributions with means $\theta^* = [.1, .2, .3, .4, .5, .6, .7, .8,
.9]$. We defined priors/posteriors to be Beta distributed with initial
parameters $\alpha = \beta = 1$, mimicking a uniform prior on the set of arms.

The posterior mass put on certain events, in particular the confidence, were
computed as described in \Cref{section:algorithm}. \Cref{fig:confidences}
indicates the confidence for a given number of steps of our method compared to
the the uniform allocation and Thompson sampling.
\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{190723-confidences.png}
  \caption{Unconstrained and constrained optimal allocation for $\theta_1$ and $\theta_2$, top-4}
  \label{fig:confidences}
\end{figure}
As we don't have a closed form for the average measurement plan
$\bar{\psi}_{n,l}$ at our disposal, we approximate it with the empirical
sampling frequencies. \Cref{fig:measurement_plan} indicates said empirical
sampling frequencies. Note that $\bar{\psi}_{S^*}$ can nicely be read to equal
$\frac{1}{2}$.
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{190723-selections_2.png}
  \caption{Unconstrained and constrained optimal allocation for $\theta_1$ and
  $\theta_2$, top-4}
  \label{fig:measurement_plan}
\end{figure}
As we know from \Cref{section:optimal_statements}, the optimal allocation
gathers equal evidence for all pairs from $S^* \times S^{*c}$. We used the
empirical sampling frequencies to approximate those coefficients. A qualitative
comparison of the individual coefficients can be found in
\Cref{fig:algorithm_coefficients}. The individual $C_{j, i}$ were computed just
as described in \Cref{section:concrete_optimal_allocation}.
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{190909-coefficients_2000.png}
  \caption{Unconstrained and constrained optimal allocation for $\theta_1$ and
  $\theta_2$, top-4}
  \label{fig:algorithm_coefficients}
\end{figure}

\section{Further results and outlook}

Looking at \Cref{proposition:optimality_sufficient_condition}, we see that in
order to show $\bar{\psi}_n \rightarrow \psi^{\frac{1}{2}*}$, we need to show
that once an arm has been oversampled on average, its likelihood of being
sampled soon is very low. We expect the upper bound from
\Cref{lemma:fraction_bound} to be useful for this task as a similar approach has
succeeded in Russo's case. In particular, the missing link in the chain is to
show that under oversampling conditions, $\min_{j \in S^*} C_{j,
i}(\bar{\psi}_j, \bar{\psi}_i) \geq \Gamma^*_{\frac{1}{2}}$. If the latter is
satisfied, the probability $\psi_{n, i}$ is exponentially small in $n$ and
therefore the sum from \Cref{proposition:optimality_sufficient_condition} finite
by the geometric series.

However, this crucial condition is not naturally satisfied. Investigating it
more carefully, we realize that its satisfaction requires an excess of evidence
over the optimal allocation. This can only happen in a given round if the
optimal arms $j \in S^*$ have sampling probabilities greater equal to the
optical allocation $\psi^{\frac{1}{2}*}$.

As we are not convinced that this holds true, we started looking into the
scenario in which the minimizing $j$ is currently undersampled. Our intuition is
that if $\hat{j} \in S^*$ is currently undersampled, it will soon be corrected
for.

\Cref{lemma:limsup_undersampling} proposes an analogous form of
\Cref{proposition:optimality_sufficient_condition} for undersampling.
\Cref{lemma:psi_undersampled} suggests that if an arm is undersampled, it will
be given a lot of measurement effort very soon.
\begin{lemma}\label{lemma:limsup_undersampling}
  Given $\sum_n(\frac{1}{2} - \psi_{n, l}) \mathbb{I}[\bar{\psi}_{n, l} \leq
    \psi_l^* - \delta] < \infty$ we have that
  \[\limsup \bar{\psi}_{n, l} \geq \psi_j^* \text{ and } \liminf \bar{\psi}_{n,
      ul} \geq \psi_j^*\]
\end{lemma}
\begin{lemma}\label{lemma:psi_undersampled}
  If $\alpha_{n, S^*} \rightarrow 1$ and $\bar{\psi}_{n, j} < \psi^{\frac{1}{2}*}_j$ then there exists $\delta > 0$
  with
  \begin{align}
    \psi_{n, \hat{j}} \geq \frac{1}{2} - m\exp(-n \delta)
  \end{align}
\end{lemma}

\section{Proofs}\label{section:txts_proofs}
\begin{proof}[\Cref{lemma:finite_measurement}]

  \begin{itemize}
  \item Note that this statement is equal for top-1 and top-$m$. Hence we can
  employ \Cref{proposition:russo_4} telling us that if $\sum_{n \in \mathbb{N}}
  \psi_{n, l} \rightarrow \infty$, it follows that
  \begin{align}
    \Pi_n(\{\theta: \theta_l \in (\theta^*_l - \epsilon, \theta^*_l +
        \epsilon)\}) \rightarrow 1 \label{eq:concentration}
  \end{align}
  Hence \eqref{eq:concentration} holds for any $l \notin \mathcal{I}$. \item If
  $\mathcal{I} = \emptyset$, every arm is sampled infinitely often when the
  number of samples goes to infinity. This means that \eqref{eq:concentration}
  holds for every arm. In other words, our estimate of each arm is arbitrarily
  concentrated around its true value, i.e. $\Pi_n(\{\theta^*\}) \rightarrow 1$.
  Recalling our definitions of $S^*$, $\Theta_S$ and $\alpha_{n, S}$, we see
  that
  \begin{align}
    \alpha_{n, S^*} = \Pi_n(\Theta_{S^*}) \geq \Pi_n(\{\theta^*\}) \rightarrow 1
  \end{align}

  As $\alpha_{n, S}$ is bounded by $[0, 1]$, we obtain the desired statement.

  \item Similar to $\theta_S$, we define $\theta_{S, \epsilon} = \{\theta \in
  \Theta | \min_{l_1 \in S} \theta_j \geq \max_{l_2 \notin S} \theta_i +
  \epsilon\}$. In other words, $\theta_{S,\epsilon}$ is the set of parameters
  under which $S$ optimal with a distance of at least $\epsilon$ to the
  next-best arm. As a lower bound suffices for our statement, we tighten this
  condition to conveniently allow for usage of the previous points.

  We now pose two tightening conditions on parameters $\theta$: First we
  restrict arms from $\mathcal{I}^c$ to be at most $\epsilon$ better than the
  \emph{true} best parameter from $\mathcal{I}^c$. Second we restrict arms from
  $\mathcal{I}$, other than from $S$, to be worse than the true best parameter
  from $\mathcal{I}^c$. For the sake of convenience, let us define $\rho^* =
  \max_{S \not\subseteq \mathcal{I}} \min_{l_1 \in S} \theta^*_{l_1}$ on the
  true parameters. For $S \subset \mathcal{I}$, we have:
  \begin{align}
    \theta_{S, \epsilon} &= \{\theta|(\min_{l_1 \in S} \theta_{l_1} \geq
        \max_{l_2 \in \mathcal{I} \setminus S} \theta_{l_1} + \epsilon) \wedge (\min_{l_1 \in S}
        \theta_{l_1} \geq\ \max_{l_2 \notin \mathcal{I}} \theta_{l_2} + \epsilon)\} \\
      &= \{\theta | (\forall l_2 \in \mathcal{I} \setminus S: \min_{l_1 \in S} \geq
          \theta_{l_2} + \epsilon) \wedge (\min_{l_1 \in S} \theta_{l_1} \geq\ \max_{l_2
          \notin \mathcal{I}} \theta_{l_2} + \epsilon)\} \\
      & \supseteq \{\theta |(\min_{l_1 \in S} \theta_{l_1} \geq \rho^* + 2 \epsilon)
          \wedge (\forall {l_2} \in \mathcal{I} \setminus S: \rho^* > \theta_{l_2})
          \wedge (\min_{l_1 \in S} \theta_{l_1} \geq\ \max_{l_2 \notin \mathcal{I}}
          \theta_{l_2} + \epsilon)\} \\
      & \supseteq \{\theta |(\min_{l_1 \in S} \theta_{l_1} \geq \rho^* + 2 \epsilon)
          \wedge (\forall l_2 \in \mathcal{I} \setminus S: \rho^* > \theta_{l_2})
          \wedge (\rho^* + \epsilon \geq \max_{l_2 \notin \mathcal{I}}
          \theta_{l_2})\} \\
      &= \underbrace{\{\theta |(\min_{l_1 \in S} \theta_{l_1} \geq \rho^* + 2
          \epsilon) \wedge (\forall l_2 \in \mathcal{I} \setminus S: \rho^* >
          \theta_{l_2}) \}}_\text{A} \setminus \underbrace{\{\theta| \max_{l_2 \notin
          \mathcal{I}} \theta_{l_2} > \rho^* + \epsilon\}}_\text{B}
  \end{align}
  Where the last step follows from the fact that $\{x|p(x) \wedge q(x)\} =
  \{x|p(x)\} \setminus \{x|\neg q(x)\}$. By \eqref{eq:concentration}, we know
  that $\Pi_n(B) \rightarrow 0$. The second part of \Cref{proposition:russo_4} tells
  us that over any open interval $(\theta_l', \theta_l'')$, we have $\inf_{n \in
  \mathbb{N}} \Pi_n(\{\theta| \theta_l \in (\theta_l', \theta_l'')\ \forall l
  \in \mathcal{I}\}) > 0$. By defining $(\theta_l', \theta_l'')$ to be $(\rho +
  2 \epsilon, \bar{\theta})$ for all arms from $S \subset \mathcal{I}$ and
  $(\underline{\theta}, \rho^* + \epsilon)$ for all arms in $\mathcal{I}
  \setminus S$, we get $\inf_{n \in \mathbb{N}} \Pi_n(A) > 0$. Together, this
  gives us
  \begin{align}
    \inf_{n \in \mathbb{N}} \alpha_{n, S} &= \inf_{n \in \mathbb{N}}
        \Pi_n(\theta_{S, \epsilon}) \geq \inf_{n \in \mathbb{N}}(\Pi_n(A) -
        \Pi_n(B)) > 0 \\
    \liminf_{n \in \mathbb{N}} \alpha_{n, S} &\geq \inf_{n \in \mathbb{N}}
        \alpha_{n, S} > 0
  \end{align}
  \end{itemize}
\end{proof}

\begin{proof}[\Cref{proposition:measurement_plan_arm}]
  It will prove itself useful to first investigate the probability of a given
  arm $l$ belonging to either $S_1$ or $S_2$, yet not both.
  \begin{align}
    &\Pr[l \in S_1 \wedge l \notin S_2] + \Pr[l \notin S_1 \wedge l \in S_2]  \\
    =& \sum_{S: l \in S} \Pr[S_1 = S] \sum_{S': l \notin S'} \Pr[S_2 = S' | S_1
        = S] + \notag\\
    & \sum_{S': l \notin S'} \Pr[S_1 = S'] \sum_{S: l \in S} \Pr[S_2 = S | S_1
        = S'] \\
    =& \sum_{S: l \in S} \alpha_{n, S} \sum_{S': l \notin S'} \frac{\alpha_{n,
        S'}}{1 - \alpha_{n, S}} + \sum_{S': l \notin S'} \alpha_{n, S'}
        \sum_{S: l \in S} \frac{\alpha_{n, S}}{1 - \alpha_{n, S'}}\\
    =& \sum_{S: l \in S} \frac{\alpha_{n, S}}{1 - \alpha_{n, S}} \sum_{S': l
        \notin S'} \alpha_{n, S'} + \sum_{S': l \notin S'} \frac{\alpha_{n,
        S'}}{1 - \alpha_{n, S'}} \sum_{S: l \in S} \alpha_{n, S}\\
    =& \sum_{S: l \in S} \frac{\alpha_{n, S}}{1 - \alpha_{n, S}} (1 -
        \alpha_{n, l}) +  \sum_{S': l \notin S'} \frac{\alpha_{n, S'}}{1 -
        \alpha_{n, S'}} \alpha_{n, l} \\
    =& (1 - \alpha_{n, l})\sum_{S: l \in S} \frac{\alpha_{n, S}}{1 - \alpha_{n,
        S}} + \alpha_{n, l} \sum_{S': l \notin S'} \frac{\alpha_{n, S'}}{1 -
        \alpha_{n, S'}}
  \end{align}
  This identity can now be leveraged for both lower and upper bound. But first,
  let's express the the measurement plan exactly.
  \begin{align}
    \psi_{n, l} =& \Pr[l \in S_1 \wedge l \notin S_2 \wedge I_n = l] + \Pr[l
        \notin S_1 \wedge l \in S_2 \wedge I_n = l] \\
    =& \Pr[I_n = l | l \in S_1 \wedge l \notin S_2] \Pr[l \in S_1 \wedge l
        \notin S_2] + \notag\\
    & \Pr[I_n = l | l \notin S_1 \wedge l \in S_2] \Pr[l \notin S_1 \wedge l
        \in S_2]
  \end{align}
  Observe that both terms $\Pr[I_n = l | l \in S_1 \wedge l \notin S_2]$ and
  $\Pr[I_n = l | l \notin S_1 \wedge l \in S_2]$ correspond to a very similar
  situation: we know that $l$ is part of the XOR, but we don't know how exactly
  the rest of the XOR looks like. To those quantities we can apply the
  aforementioned naïve bounds of $\frac{1}{2}$ and $\frac{1}{2m}$.
  \begin{align}
    \psi_{n, l} \leq& \frac{1}{2} (\Pr[l \in S_1 \wedge l \notin S_2] + \Pr[l
        \notin S_1 \wedge l \in S_2]) \\
      =& \frac{1}{2}((1 - \alpha_{n, l}) \sum_{S: l \in S} \frac{\alpha_{n,
          S}}{1 - \alpha_{n, S}} + \alpha_{n, l}) \sum_{S': l \notin S'}
          \frac{\alpha_{n, S'}}{1 - \alpha_{n, S'}} \\
    \psi_{n, l} \geq& \frac{1}{2m} (\Pr[l \in S_1 \wedge l \notin S_2] + \Pr[l
        \notin S_1 \wedge l \in S_2]) \\
      =& \frac{1}{2m}((1 - \alpha_{n, l}) \sum_{S: l \in S} \frac{\alpha_{n,
          S}}{1 - \alpha_{n, S}} + \alpha_{n, l}) \sum_{S': l \notin S'}
          \frac{\alpha_{n, S'}}{1 - \alpha_{n, S'}} \\
  \end{align}
\end{proof}

\begin{proof}[\Cref{proposition:measurement_pan_set}]
  In order to show the first equality, we rely on the idea that we only check
  if the first sampled set $S_1$ is equal to $S$.
  \begin{align}
    \psi_{n, S} &= \Pr[I_n \in S] \\
      &= \Pr[S_1 = S \wedge I_n \in S_1] + \Pr[S_1 \neq S \wedge I_n \in S] \\
      &= \Pr[I_n \in S_1 | S_1 = S] \Pr[S_1 = S] + \Pr[I_n \in S| S_1 \neq
          S]\Pr[S_1 \neq S] \\
      &= \Pr[I_n \in S_1] \Pr[S_1 = S] + \Pr[I_n \in S| S_1 \neq S](1 - \Pr[S_1
          = S]) \\
      &= \frac{1}{2} \alpha_{n, S} +  \Pr[I_n \in S | S_1 \neq S] (1 -
          \alpha_{n, S})
  \end{align}
  Note that $\Pr[I_n \in S_1] = \Pr[I_n \in S_2] = \frac{1}{2}$ as the XOR
  operation ensures that equally many elements from $S_1$ and $S_2$ in the XOR
  set. In combination with uniform sampling this yields aforementioned relation.

  In order to show the second equality, we go a step further by checking if
  either of both sets equals $S$. For that purpose we first look into the
  probability that $S$ is sampled as a second set and that $I_n$ stems from $S$.
  \begin{align}
    & \Pr[S_2 = S \wedge I_n \in S_2] \\
    &= \sum_{S'\neq S}\Pr[S_1 = S' \wedge S_2 = S \wedge I_n \in S_2] \\
    &= \sum_{S'\neq S} \Pr[S_1 = S'] \Pr[S_2 = S | S_1 = S'] \Pr[I_n \in S_2 |
        S_1 = S' \wedge] \\
    &= \sum_{S'\neq S} \alpha_{n, S'} \frac{\alpha_{n, S}}{1 - \alpha_{n, S'}}
        \frac{1}{2} = \frac{\alpha_{n, S}}{2} \sum_{S'\neq S} \frac{\alpha_{n,
        S'}}{1 - \alpha_{n, S'}}
  \end{align}
  \begin{align}
    \psi_{n, S} =& \Pr[I_n \in S] \\
      =& \Pr[S_1 = S \wedge I_n \in S_1] + \Pr[S_2 = S \wedge I_n \in S_2] +
          \Pr[S_1, S_2 \neq S \wedge I_n \in S] \\
      &= \frac{\alpha_{n, S}}{2} +  \frac{\alpha_{n, S}}{2} \sum_{S'\neq S}
          \frac{\alpha_{n, S'}}{1 - \alpha_{n, S'}} + \Pr[S_1, S_2 \neq S
          \wedge I_n \in S]
    \end{align}
\end{proof}

\begin{proof}[\Cref{lemma:infinite_measurement}]
  \Cref{proposition:measurement_pan_set} tells us that $\psi_{n, S} > \gamma
  \alpha_{n, S}$ for some constant $\gamma > 0$. Thanks to this we have $\sum_{n
  \in \mathbb{N}} \psi_{n, S} > \frac{1}{2} \sum_{n \in \mathbb{N}} \alpha_{n,
  S}$. For the sake of contradiction, assume that $\exists S'$ with $\sum_{n \in
  \mathbb{N}} \psi_{n, S'} < \infty$. According to
  \Cref{lemma:finite_measurement}'s definition, we have $S \subset \mathcal{I}$.
  Hence we can apply its third clause and get $\liminf_{n \rightarrow \infty}
  \alpha_{n, S'} > 0$. It follows directly that $\sum_{n \in \mathbb{N}}
  \alpha_{n, S} \rightarrow \infty$. Hence $\sum_{n \in \mathbb{N}} \psi_{n, S}$
  tends to infinity with growing $n$ as well, which is a contradiction.
\end{proof}

\begin{proof}[\Cref{lemma:psi_convergence}]
  In \Cref{proposition:measurement_pan_set}'s first form,  $\Pr[I_n \in S | S_1
  \neq S]$ is naturally bounded by 1. The desired statement follows immediately.
\end{proof}

\begin{proof}[\Cref{lemma:measurement_plan_bound_max}]
  \Cref{proposition:measurement_plan_arm} tells us that
  \[\psi_{n, l} \leq \frac{1}{2}((1 - \alpha_{n, l}) \sum_{S: l \in S}
      \frac{\alpha_{n, S}}{1 - \alpha_{n, S}} + \alpha_{n, l}
      \sum_{S': l \notin S'} \frac{\alpha_{n, S'}}{1 - \alpha_{n, S'}})\]
  Knowing that $\alpha_{n, S^*} \rightarrow 1$, we can bound $\psi_{n, i}$ in
  the following way:
  \begin{align}
    \psi_{n, i} \leq& \frac{1}{2}((1 - \alpha_{n, i}) \frac{\sum_{S: i \in S}
        \alpha_{n, S}}{1 - \alpha_{n, S^*}} + \alpha_{n, i} \frac{\sum_{S': i
        \notin S'} \alpha_{n, S'}}{1 - \alpha_{n, S^*}}) \label{eq:l46:1}\\
      =& \frac{1}{2}((1 - \alpha_{n, i}) \frac{\alpha_{n, i}}{1 - \alpha_{n,
          S^*}} + \alpha_{n, i} \frac{1 - \alpha_{n, i}}{1 - \alpha_{n, S^*}})\\
      \leq& \frac{1}{2} \frac{2 \alpha_{n, i} (1 - \alpha_{n, i})}{\max_{S'
          \neq S^*} \alpha_{n, S'}} \\
      \leq& \frac{\alpha_{n, i}}{\max_{S' \neq S^*} \alpha_{n, S'}}
  \end{align}
  for $i \notin S^*$ as well as
  \begin{align}
    \psi_{n, j} \leq& \frac{1 - \alpha_{n, j}}{\max_{S' \neq S^*} \alpha_{n,
        S'}}
  \end{align}
  for $j \in S^*$.

  Note that in \eqref{eq:l46:1} we used that for any $S \neq S^*$, $\alpha_{n,
  S} < \alpha_{n, S^*}$ thanks to our assumption $\alpha_{n, S^*} \rightarrow
  1$.
\end{proof}

\begin{proof}[\Cref{lemma:fraction_bound}]
  We first want to find a lower bound for the denominator and then find upper
  bounds for the respective numerator.

  \Cref{proposition:characterization} gives us that
  \[\lim_{n \rightarrow \infty} \sup - \frac{1}{n} \log \Pi_n(\Theta^c_{S^*})
      \leq \Gamma^*_{\frac{1}{2}}\]
  \Cref{lemma_lim_sup} tells us that there is a sequence $\epsilon_n > 0,
  \epsilon_n \rightarrow 0$ s.t.
  \begin{align}
    - \frac{1}{n} \log \Pi_n(\Theta^c_{S^*}) &\leq \Gamma^*_{\frac{1}{2}} +
        \epsilon_n \label{eq: epsilon}\\
   \Pi_n(\Theta^c_{S^*}) &\geq \exp\{-n (\Gamma^*_{\frac{1}{2}} + \epsilon_n)\}
  \end{align}
  We observe that $\Theta^c_{S^*} = \bigcup_{S' \neq S^*} \Theta_{S'}$. We use
  the union bound to notice that $ \max_{S' \neq S^*} \alpha_{n, S'} \leq
  \Pi(\Theta_{S^*}^c) \leq {k \choose m} \max_{S' \neq S^*} \alpha_{n, S'}$. We
  have \[\lim_{n \rightarrow \infty} \frac{1}{n} \log\frac{\alpha_{n, S}}{{k
  \choose m} \alpha_{n, S}} \rightarrow 0 \Rightarrow \alpha_{n, S} \deq {k
  \choose m} \alpha_{n, S}\] and hence by the Squeeze theorem $\max_{S' \neq
  S^*} \alpha_{n, S'} \deq \Pi(\Theta_{S^*}^c)$. Combining these two insights,
  we obtain our desired lower bound for the denominator: \[\max_{S' \neq S^*}
  \alpha_{n, S'} \deq \Pi_n(\Theta^c_{S^*}) \geq \exp\{-n
  (\Gamma^*_{\frac{1}{2}} + \epsilon_n)\}\] Let's investigate the numerators.

  For $i \notin S^*$, we have $\Theta_i \subset \bar{\Theta}_i$ and hence:
  \begin{align}
    \alpha_{n,i} =& \Pi_n(\Theta_i) \\
    \leq& \Pi_n(\bar{\Theta}_i) \\
    \deq& \exp\{-n \inf_{\theta \in \bar{\Theta}_i}
        D_{\bar{\psi}_n}(\theta^*||\theta)\} \text{ \Cref{proposition:prop5}}\\
    =& \exp\{-n \min_{j \in S^*} C_{j, i}(\bar{\psi}_j, \bar{\psi}_i)\} \text{
        (\Cref{lemma:kl_to_C}) } \label{eq: only_i}
  \end{align}
  Simultaneously leveraging both bounds we obtain:
  \begin{align}
    \frac{\alpha_{n, i}}{\max_{S' \neq S^*} \alpha_{n, S'}} &\leq \frac{\exp\{-
        n \min_{j \in S^*} C_{j, i}(\bar{\psi}_j, \bar{\psi}_i)\}}{ \exp\{-n
        (\Gamma^*_{\frac{1}{2}} + \epsilon_n)\}} \\
    &= \exp\{-n( \min_{j \in S^*} C_{j, i}(\bar{\psi}_j, \bar{\psi}_i) -
        \Gamma^*_{\frac{1}{2}} - \epsilon_n) \}
  \end{align}
  As $C$ is strictly increasing in its second argument and by the assumptions of
  the lemma $\bar{\psi}_i > \psi_i^* + \delta$, we have that for any $j \in S^*$
  $C_{j, i}(\psi_j^*, \bar{\psi}_i) = C_{j, i}(\psi_j^*, \psi_i^*) + \delta' =
  \Gamma^* + \delta'$ with $\delta' > 0$. It remains to show that a similar
  equality holds when minimizing over $j$ and with $\bar{\psi}_j$ instead of
  $\psi^*$ as first argument. One should be able to leverage $\bar{\psi}_{S^*}
  \rightarrow \frac{1}{2} = \psi_{S^*}^*$ along the way.
\end{proof}

\begin{proof}[\Cref{lemma:limsup_undersampling}]
    We first show the first statement and then use it for the second. For the
    sake of contradiction, assume that $\limsup \bar{\psi}_{n, l} <
    \psi_l^*$. This implies that there is a $n_0$ from which onward
    $\bar{psi}_{n, l} < \psi_l^*$.
    \begin{align}
      \sum_n (\frac{1}{2} - \psi_{n, l}) &= \sum_{n=1}^{n_0} (\frac{1}{2} -
            \psi_{n, l}) + \sum_{n > n_0} (\frac{1}{2} - \psi_{n, l}) \\
        &= \sum_{n=1}^{n_0} (\frac{1}{2} - \psi_j) + \sum_{n > n_0}
            (\frac{1}{2} - \psi_{n, l})\mathbb{I}[\bar{\psi}_{n, l} \leq
            \psi_l^* - \delta] \\
        &= C
    \end{align}
    Where the last line follows from the assumption and the fact that the first
    sum is a finite amount of individually finite quantities. Hence we can
    write:
    \begin{align}
      \sum_n \psi_{n, l} &= -C + \sum_n \frac{1}{2} \\
      \bar{\psi}_{n, l} &= \frac{-C}{n} + \frac{1}{2}\sum_n\frac{1}{n} \\
        &= \frac{-C}{n} + \frac{1}{2} H_n
    \end{align}
    We know that $H_n$ is asymptotically equivalent to $\log(n)$. Therefore we
    arrive at $\bar{\psi}_{n, l} \rightarrow \infty$, which is a contradiction.
    Therefore we have $\limsup \bar{\psi}_{n, l} \geq \psi_j^*$.

    The argument from \Cref{proposition:optimality_sufficient_condition} can be
    mirrored to show the second statement.
  \end{proof}

  \begin{remark}[Kevin 19/10/29]
    I'm not sure whether the indicator variable should be on $\bar{\psi}$ or
    $\psi$. I think it the proof should hold in both cases, though.
  \end{remark}

  \begin{proof}[\Cref{lemma:psi_undersampled}]
    Thanks to $\alpha_{n, S^*} \rightarrow 1$: we can apply
    \Cref{lemma:measurement_plan_bound_max}, and get for $j \in S^*$:
    \begin{align}
      \psi_{n, j} &\leq \frac{1 - \alpha_{n, j}}{1 - \alpha_{n, S^*}}
    \end{align}
    Additionally, according to \Cref{lemma:psi_convergence}, $\alpha_{n, S^*}
    \rightarrow 1$, allows us to assume that $\psi_{n, S^*} \rightarrow
    \frac{1}{2}$.

    Assume for $\hat{j} \in S^*$ it holds that $\bar{\psi}_{\hat{j}, n} <
    \psi^{\frac{1}{2}*}_{\hat{j}}$
    \begin{align}
      \psi_{n, \hat{j}} &= \psi_{n, S^*} - \sum_{j \in S^* \setminus
          \{\hat{j}\}} \psi_{n, j}\\
        &\rightarrow \frac{1}{2} - \sum_{j \in S^* \setminus \{\hat{j}\}}
            \psi_{n, j}\\
        &\geq \frac{1}{2} - \sum_{j \in S^* \setminus \{\hat{j}\}} \frac{1 -
            \alpha_{n, j}}{1 - \alpha_{n, S^*}} \text{
            (\Cref{lemma:measurement_plan_bound_max})} \\
        &= \frac{1}{2} - \sum_{j \in S^* \setminus \{\hat{j}\}}
            \frac{\Pi_n(\Theta_{m, j}^c)}{\Pi_n(\Theta_{S^*}^c)} \\
        &= \frac{1}{2} - \sum_{j \in S^* \setminus \{\hat{j}\}} \frac{\exp\{-n
            \min_{i \notin S^*} C_{j, i}(\bar{\psi}_j, \bar{\psi}_i) \}}{\exp\{-
            n \min_{i \notin S^*} \min_{j \in S^*} C_{j, i}(\bar{\psi}_j,
            \bar{\psi}_i) \}} \label{eq:like_lemma10}\\
        &= \frac{1}{2} - \sum_{j \in S^* \setminus \{\hat{j}\}} \exp\{-
            n(\min_{i \notin S^*} C_{j, i}(\bar{\psi}_j, \bar{\psi}_i) -
            \min_{i \notin S^*} \min_{j \in S^*} C_{j, i}(\bar{\psi}_j,
            \bar{\psi}_i))\} \\
        &= \frac{1}{2} - m \exp(-n\delta)
    \end{align}
    Where \Cref{eq:like_lemma10} relies on a combination of
    \Cref{proposition:prop5} and \Cref{lemma:kl_to_C}, just as we've done in
    \Cref{lemma:fraction_bound}. The last step stems from the fact that a
    minimization over both indices will always yield a value lesser or equal to
    the minimization over only one of the indices.
  \end{proof}
